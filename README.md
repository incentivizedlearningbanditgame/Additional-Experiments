The three experiments compare our algorithm IPA to a UCB baseline and to the Principal's epsilon Greedy algorithm proposed in Dogan et al., Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents. We ran the simulations on a 5 arms bandit instance, 40 000 time steps and we changed the exploration parameter m. As it can be observed on two of the experiments, for too small values of m (respectively m=30 and m=100), the algorithm underexplores and fails to converge. If m is chosen too big (2000 in our example), the algorithm explores too much and incurs an important regret.

On the captions, T is the horizon, K refers to the number of available actions, rewards_0_1 means that the rewards are taken in [0,1] and m is the experimental parameter that needs to be chosen in the Principal's epsilon greedy algorithm. The number of episodes/epochs defines the number of times each algorithm is run for T iterations. The curves represent the average and standard deviation accross all different episodes.
